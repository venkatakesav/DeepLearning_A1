{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Neural Network class\n",
    "class ThreeLayerNeuralNetwork:\n",
    "    # Initialize the class\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate, activation_function, Optimizer): \n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_function = activation_function\n",
    "        self.Optimizer = Optimizer\n",
    "        self.input_layer = np.random.randn(self.input_nodes, 721)\n",
    "        self.hidden_layer = np.random.randn(self.hidden_nodes, 721)\n",
    "        self.output_layer = np.random.randn(self.output_nodes, 721)\n",
    "\n",
    "    def change_learning_rate(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def change_activation_function(self, activation_function):\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def change_Optimizer(self, Optimizer):\n",
    "        self.Optimizer = Optimizer\n",
    "\n",
    "    def change_hidden_nodes(self, hidden_nodes):\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "\n",
    "    def _init_weight_matrices(self, input_nodes, hidden_nodes, output_nodes):\n",
    "        # Initialize the weight matrices with random values\n",
    "        self.weight_matrix_1 = np.random.randn(hidden_nodes, input_nodes)\n",
    "        self.weight_matrix_2 = np.random.randn(output_nodes, hidden_nodes)\n",
    "\n",
    "    def _init_bias_vectors(self, input_nodes, hidden_nodes, output_nodes):\n",
    "        # Initialize the Bias Vector 1\n",
    "        # The Size of the Bias Vector 1 is (hidden_nodes, 1)\n",
    "        self.bias_vector_1 = np.zeros((self.hidden_nodes, 1))\n",
    "        # The Size of the Bias Vector 2 is (output_nodes, 1)\n",
    "        self.bias_vector_2 = np.zeros((self.output_nodes, 1))\n",
    "\n",
    "    def _loss_function(self, y, y_hat):\n",
    "        # print('Loss', np.square(y - y_hat)/2)\n",
    "        # The Loss Function is MSE\n",
    "        print('Y Shape: ', y.shape)\n",
    "        return (np.sum(np.square(y - y_hat)))/(y.shape[1])\n",
    "    \n",
    "    def _loss_function_grad(self, y, y_hat):\n",
    "        return np.array(2*(y_hat - y)/y.shape[1])\n",
    "    \n",
    "    def _activation_function(self, x):\n",
    "        # Write Code for the Tanh, activation function\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def _grad_of_activation(self, x):\n",
    "        # Gradient of Activation\n",
    "        # print('Gradient_Val', 1 - np.tanh(x)**2)\n",
    "        return 1 - np.square(np.tanh(x))\n",
    "\n",
    "    def _forward_pass(self, input_vector):\n",
    "        # The Size of the Input Vector is (input_nodes, 1)\n",
    "        self.input_vector = input_vector\n",
    "        # print(\"Input Vector: \", self.input_vector)\n",
    "        # The Size of the Hidden Layer is (hidden_nodes, 1)\n",
    "        self.input_vector = self.input_vector.T\n",
    "        self.activation_layer = np.dot(self.weight_matrix_1, self.input_vector) \n",
    "        print(\"=====================================================================================================\")\n",
    "        print(self.activation_layer.shape)\n",
    "        print(\"=====================================================================================================\")\n",
    "        # print(\"Activation Layer: \", self.activation_layer)\n",
    "        self.hidden_layer = self._activation_function(self.activation_layer) + self.bias_vector_1\n",
    "        # print(\"Hiddent Layer \", self.hidden_layer)\n",
    "        # The Size of the Output Layer is (output_nodes, 1) \n",
    "        self.output_layer = np.dot(self.weight_matrix_2, self.hidden_layer) + self.bias_vector_2\n",
    "        # print('==============================================================================================') \n",
    "        print(\"Output Layer: \", self.output_layer.shape)\n",
    "        return self.output_layer\n",
    "\n",
    "    def _backward_pass(self, target_vector):\n",
    "        # Basically we need to obtain derivatives with respect to two weight matrices\n",
    "        # Weight Matrix 1 -> (hidden_nodes, input_nodes)\n",
    "        # Weight Matrix 2 -> (output_nodes, hidden_nodes)\n",
    "        # Bias Vector 1 -> (hidden_nodes, 1)\n",
    "        # Bias Vector 2 -> (output_nodes, 1)\n",
    "        target_vector = np.array(target_vector)\n",
    "        # First With Weight Matrix 2\n",
    "        # The Size of the Target Vector is (output_nodes, 1)\n",
    "        target_vector = target_vector.reshape(self.output_nodes, -1)\n",
    "        # First, we need to calculate the derivative of the loss function with respect to the output layer\n",
    "        Loss_Derivative = self._loss_function_grad(target_vector, self.output_layer)\n",
    "        # print(\"Target Vector: \", target_vector)\n",
    "        # print(\"Output Layer: \", self.output_layer)\n",
    "        # Second, we need to calculate the derivative of the output layer with respect to the weight matrix 2\n",
    "        Weight_Matrix_2_Derivative = self.hidden_layer\n",
    "        # Now, Vector Product of Loss Derivative and Weight Matrix 2 Derivative\n",
    "        Weight_Matrix_2_Derivative = np.dot(Loss_Derivative, Weight_Matrix_2_Derivative.T)\n",
    "\n",
    "        # # Now, Find the Bias Vector\n",
    "        Bias_Vector_2_Derivative = Loss_Derivative\n",
    "\n",
    "        # First, we need to calculate the derivative of the loss function with respect to the hidden layer\n",
    "        Loss_Derivative = np.dot(self.weight_matrix_2.T, Loss_Derivative)\n",
    "        # Second, we need to Calculate the derivative of the hidden layer with respect to the activation layer\n",
    "        Activation_Layer_Derivative = self._grad_of_activation(self.activation_layer)\n",
    "        # Here the Activation_Layer_Derivative is of shape (25, 1) -> Which is the same as the hidden layer -> Multiply element wise\n",
    "        Temp_Dertivative = np.multiply(Loss_Derivative, Activation_Layer_Derivative)\n",
    "        Weight_Matrix_1_Derivative = np.dot(Temp_Dertivative, self.input_vector.T)\n",
    "\n",
    "        # Now, Find the Bias Vector\n",
    "        Bias_Vector_1_Derivative = Temp_Dertivative\n",
    "        \n",
    "        # Now, we have the derivatives of the loss function with respect to the weight matrices\n",
    "        # Now, we need to update the weight matrices\n",
    "        # Update the Weight Matrix 1\n",
    "        self.weight_matrix_1 = self.weight_matrix_1 - self.learning_rate * Weight_Matrix_1_Derivative\n",
    "        self.bias_vector_1 = self.bias_vector_1 - self.learning_rate * Bias_Vector_1_Derivative\n",
    "        # Update the Weight Matrix 2\n",
    "        self.weight_matrix_2 = self.weight_matrix_2 - self.learning_rate * Weight_Matrix_2_Derivative\n",
    "        self.bias_vector_2 = self.bias_vector_2 - self.learning_rate * Bias_Vector_2_Derivative\n",
    "\n",
    "        # # Clear the Derivative Variables\n",
    "        Weight_Matrix_1_Derivative = None\n",
    "        Weight_Matrix_2_Derivative = None\n",
    "\n",
    "    def Train(self, train_data_x, train_data_y, epochs):\n",
    "        # Initialize the weights first\n",
    "        self._init_weight_matrices(self.input_nodes, self.hidden_nodes, self.output_nodes)\n",
    "        # Initialize the bias first\n",
    "        self._init_bias_vectors(self.input_nodes, self.hidden_nodes, self.output_nodes)\n",
    "        # Now, we need to train the model\n",
    "        # We need to iterate over the training data\n",
    "        # Plot the Losses Dynamically \n",
    "\n",
    "        # Create a figure and axis for dynamic plotting\n",
    "        plt.ion()  # Turn on interactive mode for dynamic updates\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Train the model\n",
    "            self._forward_pass(train_data_x)\n",
    "            self._backward_pass(train_data_y)\n",
    "\n",
    "            # Create a copy of the train_data_y\n",
    "            train_data_y_copy = train_data_y.copy()\n",
    "            train_data_y_copy = train_data_y_copy.reshape(-1, 1)\n",
    "            train_data_y_copy = train_data_y_copy.T\n",
    "            print(train_data_y_copy.shape)\n",
    "\n",
    "            loss = self._loss_function(train_data_y_copy, self.output_layer.T)\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Print the loss\n",
    "            print('Epoch: ', epoch, 'Loss: ', loss)\n",
    "\n",
    "        ax.clear()\n",
    "        ax.plot(losses, label='Loss')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        plt.pause(0.1)  # Pause for a short time to allow the plot to update\n",
    "        \n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "\n",
    "    def Test(self, data_point):\n",
    "        # We need to iterate over the test data\n",
    "        return self._forward_pass(data_point)\n",
    "\n",
    "    def Predict(self, data_point):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "data = pd.read_csv('./Concrete_Data.csv', header=None)\n",
    "# Remove the First Row\n",
    "data = data.iloc[1:]\n",
    "# Print the Shape of the Data\n",
    "print(data.shape)\n",
    "\n",
    "# Target Variable \n",
    "target_variable = data.iloc[:, -1]\n",
    "# Input Variables\n",
    "input_variables = data.iloc[:, 0:8]\n",
    "\n",
    "# Convert both to numpy arrays\n",
    "target_variable = np.array(target_variable, dtype=np.float32)\n",
    "input_variables = np.array(input_variables, dtype=np.float32)\n",
    "\n",
    "print(target_variable.shape)\n",
    "print(input_variables.shape)\n",
    "\n",
    "# Preprocess the Data\n",
    "########################\n",
    "\n",
    "########################\n",
    "\n",
    "# Split the Data into Train and Test\n",
    "# Randomly choose 70% and 30% of the data\n",
    "Total_samples = target_variable.shape[0]\n",
    "Train_percentage = 0.7\n",
    "\n",
    "Train_Samples = 0.7*Total_samples\n",
    "Test_Samples = 0.3*Total_samples\n",
    "\n",
    "# Shuffle The Data\n",
    "indices = np.arange(Total_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split this data\n",
    "Train_indices = indices[:int(Train_Samples)]\n",
    "Test_indices = indices[int(Train_Samples):]\n",
    "\n",
    "X_Train = input_variables[Train_indices]\n",
    "Y_Train = target_variable[Train_indices]\n",
    "x_test = input_variables[Test_indices]\n",
    "y_test = target_variable[Test_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Instance of The N.N class\n",
    "Neural_Network = ThreeLayerNeuralNetwork(input_nodes=8, hidden_nodes=50, output_nodes=1, learning_rate=0.001, activation_function='sigmoid', Optimizer='SGD')\n",
    "\n",
    "# # Normalize the Data\n",
    "# X_Train = X_Train/np.amax(X_Train, axis=0)\n",
    "# Y_Train = Y_Train/np.amax(Y_Train, axis=0)\n",
    "\n",
    "Neural_Network.Train(X_Train, Y_Train, epochs=1000)\n",
    "\n",
    "# y_pred = []\n",
    "\n",
    "# # Find the Loss on the Test Data\n",
    "# for i in range(len(x_test)):\n",
    "#     y_pred.append(Neural_Network.Test(x_test[i]))\n",
    "\n",
    "# # Calculate the Loss\n",
    "# Loss = 0\n",
    "# for i in range(len(y_test)):\n",
    "#     Loss += Neural_Network._loss_function(y_test[i], y_pred[i])\n",
    "\n",
    "# print('Loss on Test Data: ', Loss/len(y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
